# -*- coding: utf-8 -*-
"""resnet_50.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uu1ooVwzs1i_mkfEy7veDbdhUzoJ-hNZ
"""
<<<<<<< HEAD
"""
||||||| merged common ancestors
Resnet
"""
=======
>>>>>>> a_play

<<<<<<< HEAD
#%%
||||||| merged common ancestors
#%% #Imports
=======

>>>>>>> a_play
import os
import datetime
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, Input
import tensorboard
from tensorboard.plugins.hparams import api as hp
<<<<<<< HEAD
from src.preprocessing.image_gen import MultiTaskImageGen2, BalanceImageGenerator
||||||| merged common ancestors
from src.preprocessing.image_gen import MultiTaskImageGen, BalanceImageGenerator
=======
from src.preprocessing.image_gen import MultiTaskImageGen2, BalanceImageGenerator, MultiTaskImageGen
from src.preprocessing.dataset_stuff import augment, separate_images
>>>>>>> a_play
from src.applications.resnet_custom import make_model

<<<<<<< HEAD
#%%
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
||||||| merged common ancestors
#%% #Check for TPU
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='fyp',
                                                          zone='europe-west4-a',
                                                          project='eeefyp')
=======
# tf.__version__

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
>>>>>>> a_play
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
<<<<<<< HEAD
strategy = tf.distribute.experimental.TPUStrategy(resolver)

#%%
||||||| merged common ancestors
strategy=tf.distribute.experimental.TPUStrategy(resolver)                                                          
# try:
  # resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='fyp',
  #                                                         zone='europe-west4-a',
  #                                                         project='eeefyp') # TPU detection
# except ValueError: # If TPU not found
#   tpu = None 

# if tpu:
#   tf.config.experimental_connect_to_cluster(resolver)
#   tf.tpu.experimental.initialize_tpu_system(tpu)
#   strategy = tf.distribute.experimental.TPUStrategy(tpu, steps_per_run=128)
#   print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])  
# else:
#   strategy = tf.distribute.get_strategy() # Default strategy that works on CPU and single GPU
#   print('Running default instead')
# print("Number of accelerators: ", strategy.num_replicas_in_sync)

#%% #Global vsrs
=======
strategy = tf.distribute.experimental.TPUStrategy(resolver)

>>>>>>> a_play
TIMESTAMP = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
AUTOTUNE = tf.data.experimental.AUTOTUNE
DATADIR = 'gs://alexfyp_data'
LOGDIR = 'gs://alexfyp_logs'
BATCH_SIZE = 128
CLASS_LABELS = ['faw', 'zinc_def', 'nlb', 'healthy']
<<<<<<< HEAD

EPOCHS=100
num_nlb = 14570
STEPS_PER_EPOCH = np.ceil(3*0.8*0.8*num_nlb/BATCH_SIZE)

#%%
feature_description = {
    'rows': tf.io.FixedLenFeature([1], tf.int64),
    'cols': tf.io.FixedLenFeature([1], tf.int64),
    'channels': tf.io.FixedLenFeature([1], tf.int64),
    'image': tf.io.FixedLenFeature([1], tf.string),
    'labels': tf.io.VarLenFeature(tf.float32)
}

#%%
train_files = [os.path.join(DATADIR, f'sharded/train/train{index}.tfrecord') for index in range(16)]
test_files = [os.path.join(DATADIR, f'sharded/test/test{index}.tfrecord') for index in range(8)]
val_files = [os.path.join(DATADIR, f'sharded/val/val{index}.tfrecord') for index in range(8)]

#%%
train_ds = MultiTaskImageGen2(train_files, feature_description)
val_ds = MultiTaskImageGen2(val_files, feature_description)
test_ds = MultiTaskImageGen2(test_files, feature_description)

#%%
train_ds = train_ds.get_all().shuffle(2048).batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)
val_ds = val_ds.get_all().shuffle(1024).batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)
test_ds = test_ds.get_all().shuffle(1024).batch(BATCH_SIZE, drop_remainder=True)

#%%
def train_generator():
  for X, (y1, y2, y3) in train_ds:
    yield X.numpy(), (y1.numpy(), y2.numpy(), y3.numpy())

def val_generator():
  for X, (y1, y2, y3) in val_ds:
    yield X.numpy(), (y1.numpy(), y2.numpy(), y3.numpy())

def test_generator():
  for X, (y1, y2, y3) in test_ds:
    yield X.numpy(), [y1.numpy(), y2.numpy(), y3.numpy()]
||||||| merged common ancestors
CLASS_LABELS

#%% #Dataset initialisation
ds_faw = MultiTaskImageGen(os.path.join(DATADIR, 'final/faw'), 256, CLASS_LABELS)
test_faw, val_faw = ds_faw.split_dataset()

ds_zinc = MultiTaskImageGen(os.path.join(DATADIR, 'final/zinc_def'), 256, CLASS_LABELS)
test_zinc, val_zinc = ds_zinc.split_dataset()

ds_nlb = MultiTaskImageGen(os.path.join(DATADIR, 'NLB/nlb'), 256, CLASS_LABELS)
test_nlb, val_nlb = ds_nlb.split_dataset()

ds_healthy = MultiTaskImageGen(os.path.join(DATADIR, 'NLB/healthy'), 256, CLASS_LABELS)
test_healthy, val_healthy = ds_healthy.split_dataset()

#%% #Test set
test = test_faw.concatenate(test_healthy)
test = test.concatenate(test_zinc)
test = test.concatenate(test_nlb).shuffle(1000)
test = test.batch(2*BATCH_SIZE)

#%% #Validation set
val = val_faw.concatenate(val_healthy)
val = val.concatenate(val_zinc)
val = val.concatenate(val_healthy).shuffle(1000).cache()
val = val.batch(2*BATCH_SIZE)

#%%
num_nlb = 14570
STEPS_PER_EPOCH = np.ceil(3*0.8*0.8*num_nlb/BATCH_SIZE)
print(num_nlb, STEPS_PER_EPOCH)

#%%
balance_ds = BalanceImageGenerator(BATCH_SIZE, ds_faw(), ds_healthy(), ds_zinc(), ds_nlb())()

#%%
initializer = tf.keras.initializers.he_normal()
loss = tf.keras.losses.BinaryCrossentropy()
optimizer = tf.keras.optimizers.Adam()
METRICS = [tf.keras.metrics.BinaryAccuracy(name='acc'),
            tf.keras.metrics.Precision(name='psn'),
            tf.keras.metrics.Recall(name='rcl'),
            tf.keras.metrics.AUC(name='AUC')]

#%%
with strategy.scope():
    model = make_model((256,256,3), METRICS, optimizer, loss, initializer)
=======
>>>>>>> a_play

<<<<<<< HEAD
#%%
# options = tf.data.Options()
# options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.AUTO
# train_ds = train_ds.with_options(options)
# val_ds = val_ds.with_options(options)
# test_ds = test_ds.with_options(options)
||||||| merged common ancestors
#%%
model.fit(balance_ds,
          epochs=100,
          steps_per_epoch=STEPS_PER_EPOCH,
          validation_data=val,
          callbacks=[tf.keras.callbacks.TensorBoard(log_dir=LOGDIR, histogram_freq=1),
                     tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)])
=======
EPOCHS=200
>>>>>>> a_play

<<<<<<< HEAD
#%%
# with strategy.scope():
#   initializer = tf.keras.initializers.he_normal()
#   loss = tf.keras.losses.BinaryCrossentropy()
#   optimizer = tf.keras.optimizers.Adam()
#   METRICS = [tf.keras.metrics.BinaryAccuracy(name='acc'),
#               tf.keras.metrics.Precision(name='psn'),
#               tf.keras.metrics.Recall(name='rcl'),
#               tf.keras.metrics.AUC(name='AUC')]
#   model = make_model((256,256,3), METRICS, optimizer, loss, weights_initializer=initializer)

# model.summary()
||||||| merged common ancestors
#%%
model.save("gs://eeefyp/models/resnet50", include_optimizer=False)
=======
feature_description = {
    'rows': tf.io.FixedLenFeature([1], tf.int64),
    'cols': tf.io.FixedLenFeature([1], tf.int64),
    'channels': tf.io.FixedLenFeature([1], tf.int64),
    'image': tf.io.FixedLenFeature([1], tf.string),
    'labels': tf.io.VarLenFeature(tf.float32)
}
>>>>>>> a_play

<<<<<<< HEAD
#%%
model.fit(train_generator(),
          epochs=EPOCHS,
          steps_per_epoch=STEPS_PER_EPOCH,
          validation_data=val_generator())
||||||| merged common ancestors
#%% #Evaluate model
model.evaluate(test, callbacks=[tf.keras.callbacks.TensorBoard(log_dir=LOGDIR)])
=======
train_files = [os.path.join(DATADIR, f'sharded/train/train{index}.tfrecord') for index in range(16)]
test_files = [os.path.join(DATADIR, f'sharded/test/test{index}.tfrecord') for index in range(8)]
val_files = [os.path.join(DATADIR, f'sharded/val/val{index}.tfrecord') for index in range(8)]

train = MultiTaskImageGen2(train_files, feature_description)
val = MultiTaskImageGen2(val_files, feature_description)
test = MultiTaskImageGen2(test_files, feature_description)

STEPS_PER_EPOCH = 39534//BATCH_SIZE
VAL_STEPS = 1921//BATCH_SIZE
TEST_STEPS = 4157/BATCH_SIZE
print(STEPS_PER_EPOCH, VAL_STEPS, TEST_STEPS)
print(train.get_num_images(), val.get_num_images(), test.get_num_images())

train_ds = train.get_all().cache().repeat().map(augment, num_parallel_calls=AUTOTUNE)\
                          .shuffle(2048).batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)
val_ds = val.get_all().cache().shuffle(1024).batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)
test_ds = test.get_all().shuffle(1024).batch(BATCH_SIZE, drop_remainder=True)

train_img = train.get_all().map(separate_images, num_parallel_calls=AUTOTUNE)

with strategy.scope():
  # norm = tf.keras.layers.experimental.preprocessing.Normalization(axis=(0,1,2))
  # norm.adapt(train_img)
  initializer = tf.keras.initializers.he_normal()
  loss = tf.keras.losses.BinaryCrossentropy()
  optimizer = tf.keras.optimizers.Adam()
  
  METRICS = [tf.keras.metrics.BinaryAccuracy(name='acc'),
             tf.keras.metrics.Precision(name='psn'),
             tf.keras.metrics.Recall(name='rcl'),
             tf.keras.metrics.AUC(name='AUC')]

  model = make_model((256,256,3), METRICS, optimizer, loss, 0.0,
                     input_normalizer=None, weights_initializer=initializer,
                     extra_layers=True)

model.summary()

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir gs://alexfyp_logs/resnet_adam2

logdir = os.path.join(LOGDIR, 'resnet_adam2')

reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,
                              patience=5)
checkpoint = tf.keras.callbacks.ModelCheckpoint('gs://alexfyp_models/resnet_adam2',
                  monitor='val_loss', verbose=0, save_best_only=True,
                  save_weights_only=False, mode='auto', save_freq='epoch')
log_tens = tf.keras.callbacks.TensorBoard(log_dir=logdir, write_graph=False)

model.fit(train_ds,
          epochs=EPOCHS,
          steps_per_epoch=307,
          validation_data=val_ds,
          callbacks=[reduce_lr, checkpoint, log_tens])

new_model = tf.keras.models.load_model('gs://alexfyp_models/resnet_adam2')

history = new_model.evaluate(test_ds)
history

hist = {}
hist['faw_acc'] = history[4]
hist['faw_psn'] = history[5]
hist['faw_rcl'] = history[6]
hist['faw_AUC'] = history[7]
hist['zinc_acc'] = history[8]
hist['zinc_psn'] = history[9]
hist['zinc_rcl'] = history[10]
hist['zinc_AUC'] = history[11]
hist['nlb_acc'] = history[12]
hist['nlb_psn'] = history[13]
hist['nlb_rcl'] = history[14]
hist['nlb_AUC'] = history[15]

hist
>>>>>>> a_play
